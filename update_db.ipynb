{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "import time\n",
    "from calendar import monthrange\n",
    "from time import sleep\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "from pyquery import PyQuery\n",
    "import lxml\n",
    "\n",
    "REQUEST_INTERVAL = 0.01    # Length of delay between each request in seconds, to avoid server banning the spider\n",
    "\n",
    "# Date related settings and help functions\n",
    "\n",
    "YEAR_START   = 2015     ## Important: set these dates to the date of last scrape\n",
    "MONTH_START  = 10       ## If you don't set, the scraper will start from this date, which should have few\n",
    "DAY_START    = 29       ## negative effects.\n",
    "\n",
    "YEAR_END     = date.today().year\n",
    "MONTH_END    = date.today().month\n",
    "DAY_END      = date.today().day\n",
    "\n",
    "def normalizeDate(date_unit):\n",
    "    result = str(date_unit)\n",
    "    if len(result) == 1:\n",
    "        result = '0' + result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Networking helper functions\n",
    "\n",
    "BASE_URL = 'http://cctv.cntv.cn/lm/xinwenlianbo/'\n",
    "URL_SUFFIX = '.shtml'\n",
    "\n",
    "def getIndexURL(year, month, day, base_url=BASE_URL, concat_f = None, suffix = URL_SUFFIX):\n",
    "    \n",
    "    if concat_f == None:\n",
    "        concat_f = lambda year, month, day: normalizeDate(year) + normalizeDate(month) + normalizeDate(day)\n",
    "    \n",
    "    return base_url + concat_f(year, month, day) + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a list of index pages for each day\n",
      "\n",
      "The year is 2015 | first month= 10 | last month= 11\n",
      " the month is 10 | first day is 29 | last day is 31\n",
      " the month is 11 | first day is 01 | last day is 01\n"
     ]
    }
   ],
   "source": [
    "# Generate the list of index pages\n",
    "\n",
    "index_list = [];\n",
    "\n",
    "print('Generating a list of index pages for each day')\n",
    "\n",
    "for year in range(YEAR_START, YEAR_END + 1):\n",
    "    \n",
    "    first_month = MONTH_START if year == YEAR_START else  1\n",
    "    last_month  = MONTH_END   if year == YEAR_END   else 12\n",
    "    \n",
    "    print('\\nThe year is', year, '|',\n",
    "          'first month=', normalizeDate(first_month), '|',\n",
    "          'last month=', normalizeDate(last_month))\n",
    "\n",
    "    for month in range(first_month, last_month + 1):\n",
    "              \n",
    "        first_day = 1\n",
    "        _, last_day = monthrange(year, month)\n",
    "              \n",
    "        if year == YEAR_START and month == MONTH_START:\n",
    "            first_day = DAY_START\n",
    "        \n",
    "        if year == YEAR_END and month == MONTH_END:\n",
    "            last_day = DAY_END\n",
    "        \n",
    "        print('', 'the month is', normalizeDate(month), '|',\n",
    "              'first day is', normalizeDate(first_day), '|',\n",
    "              'last day is', normalizeDate(last_day))\n",
    "            \n",
    "        for day in range(first_day, last_day + 1):\n",
    "            indexURL = getIndexURL(year, month, day)\n",
    "            index_list.append(indexURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_CODING = 'utf-8'\n",
    "\n",
    "def getEncoding(html, mode='xinwenlianbo'):\n",
    "\n",
    "    if mode == 'xinwenlianbo':\n",
    "        \n",
    "        # Extract encoding information from declarations like:\n",
    "        # <meta http-equiv=\"Content-Type\" content=\"text/html; charset=gbk\" />\n",
    "        \n",
    "        CHARSET_LEFT_MARKER = 'charset='\n",
    "        CHARSET_RIGHT_MARKER = '\"'\n",
    "        charset_start_pos = html.find(CHARSET_LEFT_MARKER)\n",
    "        charset_end_pos = html.find(CHARSET_RIGHT_MARKER, charset_start_pos)\n",
    "        \n",
    "        return html[charset_start_pos + len(CHARSET_LEFT_MARKER) : charset_end_pos]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown mode of operation:\" +  mode)\n",
    "\n",
    "def getHTML(url, encoding=DEFAULT_CODING):\n",
    "    \n",
    "    # Get the HTML from a URL\n",
    "    req = requests.get(url)\n",
    "    req.encoding = getEncoding(req.text) # Requests can't correctly guess encoding\n",
    "    if req.status_code == 200:\n",
    "        return req.text\n",
    "    else:\n",
    "        raise RuntimeError('Fail to get url ' + url + ' | Status code=' + str(req.status_code))\n",
    "\n",
    "def getDateFromURL(url, mode='xinwenlianbo-index'):\n",
    "    \n",
    "    if mode == 'xinwenlianbo-index':\n",
    "        date_start = url.rfind('/')\n",
    "        date_end = url.rfind('.shtml')\n",
    "        date = url[date_start+1:date_end]\n",
    "    \n",
    "    elif mode == 'xinwenlianbo-post':\n",
    "        pass\n",
    "    \n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded http://cctv.cntv.cn/lm/xinwenlianbo/20151029.shtml\n",
      "Downloaded http://cctv.cntv.cn/lm/xinwenlianbo/20151030.shtml\n",
      "Downloaded http://cctv.cntv.cn/lm/xinwenlianbo/20151031.shtml\n",
      "Fail to get url http://cctv.cntv.cn/lm/xinwenlianbo/20151101.shtml | Status code=404\n",
      "Downloaded http://cctv.cntv.cn/lm/xinwenlianbo/20151101.shtml\n",
      "Downloaded 4 pages in total\n"
     ]
    }
   ],
   "source": [
    "# Download all index pages and save them onto local storage\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in index_list:\n",
    "    \n",
    "    try:\n",
    "        html = getHTML(url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    date_url = getDateFromURL(url)\n",
    "        \n",
    "    f = open('index_pages/xinwenlianbo_index_' + date_url + '.html', 'w', encoding='utf-8');\n",
    "    f.write(html)\n",
    "    f.close()\n",
    "    \n",
    "    print('Downloaded ' + url)\n",
    "        \n",
    "    count += 1\n",
    "    sleep(REQUEST_INTERVAL)\n",
    "\n",
    "print('Downloaded ' + str(count) + ' pages in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing the 0th index pages: index_pages/xinwenlianbo_index_20151030.html\n",
      "\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446205684032606.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446203525702208.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446203525467197.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446203881777530.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446203881941541.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204055853580.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204055681569.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204241623624.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204415429653.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204415754672.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204640028719.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204639852708.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204776436750.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446204776275744.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446205139142144.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/30/VIDE1446205138800125.shtml\n",
      "\n",
      " Processing the 1th index pages: index_pages/xinwenlianbo_index_20151031.html\n",
      "\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446291719840479.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446289922594640.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446290282670794.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446290456260865.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446290282469783.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446290456597887.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446290456754898.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446290644342194.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446291096132997.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446292255424901.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446291536351388.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446291536518398.shtml\n",
      "        Downloading http://news.cntv.cn/2015/10/31/VIDE1446291721917557.shtml\n",
      "**Downloaded 29 posts in total**\n"
     ]
    }
   ],
   "source": [
    "# Download each post from an index page\n",
    "# Currently, these functions only work on the Xinwen Lianbo site as structured in October 2015\n",
    "\n",
    "def getPostUrl(html, mode='xinwenlianbo-b'):\n",
    "    \n",
    "    # return a list of urls from an html\n",
    "    \n",
    "    post_url_list = []\n",
    "    \n",
    "    if mode == 'xinwenlianbo-b':\n",
    "        \n",
    "        URL_START_MARKER = 'http://'\n",
    "        POST_LIST_MARKER_JS = 'new title_array_01'\n",
    "        \n",
    "        # Xinwei Lianbo posts are generated with different means, and requires different extraction methods.\n",
    "        # The break point seems to be 2013-07-15. Before and on that date, JavaScript; afterwards, server side.\n",
    "        \n",
    "        #################################################\n",
    "        #                                               #\n",
    "        # The post anchors are inserted with JavaScript #\n",
    "        # Period B1                                     #\n",
    "        #                                               #\n",
    "        #################################################\n",
    "              \n",
    "        if POST_LIST_MARKER_JS in html:\n",
    "\n",
    "            marker_start_pos_list = [marker.start() for marker in re.finditer(POST_LIST_MARKER_JS, html)]\n",
    "\n",
    "            for start_pos in marker_start_pos_list:\n",
    "                url_start_pos = html.find(URL_START_MARKER, start_pos)\n",
    "                url_end_pos  = html.find(URL_SUFFIX, start_pos)\n",
    "                post_url_list.append(html[url_start_pos:url_end_pos] + URL_SUFFIX)\n",
    "        \n",
    "        ######################################################\n",
    "        #                                                    #\n",
    "        # The post anchors are generated on the server side  #\n",
    "        # Period B2                                          #\n",
    "        #                                                    #\n",
    "        ######################################################\n",
    "        \n",
    "        else:\n",
    "            d = PyQuery(html)\n",
    "            for post_anchor in d('ul.title2 a'):\n",
    "                post_url_list.append(post_anchor.attrib['href'])\n",
    "        \n",
    "        return post_url_list\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown mode of operation:\" +  mode)\n",
    "\n",
    "\n",
    "def getTitle(html, mode='xinwenlianbo-b'):\n",
    "    \n",
    "    BOILERPLATES = ('[视频]', '_新闻频道_央视网(cctv.com)')\n",
    "    \n",
    "    if mode == 'xinwenlianbo-b':\n",
    "        \n",
    "        TITLE_MARKER_JS = 'document.write(\"<title>'\n",
    "        TITLE_END_MARKER = '\"+'\n",
    "        \n",
    "        # Dynamically inserted <title>\n",
    "        if TITLE_MARKER_JS in html:\n",
    "            title_start_pos = html.find(TITLE_MARKER_JS)\n",
    "            title_end_pos = html.find(TITLE_END_MARKER, title_start_pos)\n",
    "            title = html[title_start_pos + len(TITLE_MARKER_JS) : title_end_pos]\n",
    "            \n",
    "        # <title> is generated by server\n",
    "        else:\n",
    "            try:\n",
    "                d = PyQuery(html)\n",
    "                title = d('title')[0].text\n",
    "            except IndexError:\n",
    "                # A rare condition where the title dynamically generated\n",
    "                TITLE_STARTING_TAG = '<title>'\n",
    "                TITLE_CLOSING_TAG = '</title>'\n",
    "                pos_start = html.rfind(TITLE_STARTING_TAG)\n",
    "                pos_end = html.find(TITLE_CLOSING_TAG, pos_start+1)\n",
    "                title = html[pos_start+len(TITLE_STARTING_TAG):pos_end]\n",
    "                \n",
    "        for plate in BOILERPLATES:\n",
    "            try:\n",
    "                title = title.replace(plate, '')\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "        return title\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown mode of operation:\" +  mode)\n",
    "    \n",
    "def getMainText(html, mode='xinwenlianbo-b'):\n",
    "    \n",
    "    # Return the main text \n",
    "        \n",
    "    if mode == 'xinwenlianbo-b':\n",
    "        \n",
    "        GARBAGES = ('var para_count=1', )\n",
    "        \n",
    "        d = PyQuery(html)\n",
    "        text = lxml.etree.tostring(d('div#content_body')[0], method='text', encoding='utf-8').decode('utf-8')\n",
    "       \n",
    "        for garbage in GARBAGES:\n",
    "            text =text.replace(garbage, '')\n",
    "\n",
    "        text = ''.join([line.strip() for line in text.splitlines()])\n",
    "\n",
    "        return text\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown mode of operation:\" +  mode)\n",
    "\n",
    "        \n",
    "def downloadAllPostsFromIndexHTML(html_index, index_path, mode='xinwenlianbo-b'):\n",
    "    \n",
    "    def getDateFromIndexPath(path):       \n",
    "        \n",
    "        # Extract date from path like /index_pages/xinwenlianbo_index_20110406.html\n",
    "        \n",
    "        return re.search('\\_20.*\\.', path).group(0)[1:-1]\n",
    "    \n",
    "    global post_count\n",
    "\n",
    "    for url_post in getPostUrl(html_index):\n",
    "        \n",
    "        try:\n",
    "            html_post = getHTML(url_post)      \n",
    "        except RuntimeError as error:\n",
    "            print(error)\n",
    "            \n",
    "        title = getTitle(html_post)\n",
    "        main_text = getMainText(html_post)\n",
    "\n",
    "        path = requests.utils.quote(url_post, safe='')\n",
    "        \n",
    "        print('        Downloading ' + url_post)\n",
    "\n",
    "        date_index = getDateFromIndexPath(index_path)\n",
    "        year, month, day = date_index[0:4], date_index[4:6], date_index[6:8]\n",
    "        \n",
    "        date_today = str(time.time())\n",
    "           \n",
    "        # Group files by year\n",
    "        directory = year\n",
    "        \n",
    "        if not os.path.exists('posts/' + directory):\n",
    "            os.makedirs('posts/' + directory)        \n",
    "      \n",
    "        # Save original post in HTML (encoded with utf-8)\n",
    "        f_post = open('posts/' + directory + '/post_'+ date_index + '_' + 'visited=' + date_today + '_' + path,\n",
    "                      'w',\n",
    "                      encoding='utf-8')\n",
    "        f_post.write(html_post)\n",
    "        f_post.close()\n",
    "        \n",
    "        # Save the title and main text\n",
    "        if not os.path.exists('texts/' + directory):\n",
    "            os.makedirs('texts/' + directory)        \n",
    "        \n",
    "        f_text = open('texts/' + directory + '/text_' + date_index + '_' + 'visited=' + date_today + '_' + path + '.txt',\n",
    "                      'w',\n",
    "                      encoding='utf-8')\n",
    "        f_text.write(title + '\\n' + main_text)\n",
    "        f_text.close()\n",
    "        \n",
    "        post_count += 1;\n",
    "        \n",
    "index_count = 0;\n",
    "post_count = 0;\n",
    "            \n",
    "def is_date_between(target_date, start_date, end_date):\n",
    "    # All parameters should be strings in the form of 20150130\n",
    "    assert(len(target_date) == 8)\n",
    "    assert(len(start_date) == 8)\n",
    "    assert(len(end_date) == 8)\n",
    "    \n",
    "    return start_date < target_date < end_date\n",
    "    \n",
    "for path in glob('index_pages/*.html'):\n",
    "    \n",
    "    # Ensure only download for the new index files since last update\n",
    "    \n",
    "    file_date_string = path.split('_')[-1].split('.')[0]\n",
    "    start_date_string = normalizeDate(YEAR_START) + normalizeDate(MONTH_START) + normalizeDate(DAY_START)\n",
    "    end_date_string = normalizeDate(YEAR_END) + normalizeDate(MONTH_END) + normalizeDate(DAY_END)\n",
    "\n",
    "    if is_date_between(file_date_string, start_date_string, end_date_string):\n",
    "        try:\n",
    "            print('\\n Processing the ' + str(index_count) + 'th index pages: ' + path + '\\n')\n",
    "            file_index = open(path, 'r', encoding='utf-8')\n",
    "            html_index = file_index.read()\n",
    "            downloadAllPostsFromIndexHTML(html_index, path)    \n",
    "            index_count += 1\n",
    "\n",
    "        except:\n",
    "            print(sys.exc_info()) # Prevent problems from stopping the whole scrapping process\n",
    "        \n",
    "print('**Downloaded ' + str(post_count) + ' posts in total**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
